#!/bin/bash
# å¯åŠ¨æœ¬åœ°æ¨¡åž‹æœåŠ¡ (Azure Server)
# åŒ…æ‹¬: LLM (vLLM), Embedding (infinity_emb)

set -e

echo "=========================================="
echo "Starting Local Model Services"
echo "=========================================="
echo ""

# è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
echo "âœ“ HF_ENDPOINT=$HF_ENDPOINT"

# æ£€æŸ¥ GPU
echo ""
echo "ðŸ“Š GPU çŠ¶æ€:"
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader

# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/tts
source ~/miniconda3/bin/activate

echo ""
echo "=========================================="
echo "1. Starting LLM Service (vLLM)"
echo "=========================================="
echo ""

# å®‰è£… vLLM (å¦‚æžœè¿˜æ²¡å®‰è£…)
if ! python -c "import vllm" 2>/dev/null; then
    echo "Installing vLLM..."
    pip install vllm -q
fi

# å¯åŠ¨ vLLM æœåŠ¡ (ä½¿ç”¨ GPU 0,1 åš tensor parallelism)
echo "ðŸš€ Starting vLLM server on GPU 0,1..."
echo "   Model: Qwen/Qwen3-8B (tensor-parallel-size=2)"
echo "   Port: 8000"
echo ""

# ä½¿ç”¨ 2 å¼  GPU åš tensor parallelismï¼Œé¿å…å•å¡ OOM
# V100 æ˜¯ sm70ï¼Œä¸æ”¯æŒ FlashInfer å’Œ Flash Attention
# ä½¿ç”¨ TORCH_SDPA backend (PyTorch åŽŸç”Ÿï¼ŒV100 å…¼å®¹)
# vLLM 0.11.0 å¿…é¡»ä½¿ç”¨ V1 å¼•æ“Ž
export VLLM_ATTENTION_BACKEND=TORCH_SDPA
export VLLM_USE_FLASHINFER_SAMPLER=0

nohup python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-8B \
    --served-model-name Qwen/Qwen3-8B \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.90 \
    --max-model-len 16384 \
    --disable-log-requests \
    --enforce-eager \
    --reasoning-parser qwen3 \
    > logs/vllm.log 2>&1 &

VLLM_PID=$!
echo "âœ“ vLLM started (PID: $VLLM_PID)"
echo "  Log: logs/vllm.log"

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo ""
echo "â³ Waiting for vLLM to be ready..."
sleep 30

# æµ‹è¯• LLM
echo "ðŸ§ª Testing LLM service..."
curl -s http://localhost:8000/v1/models | python -m json.tool | head -20 || echo "âš ï¸  vLLM not ready yet"

echo ""
echo "=========================================="
echo "âœ… LLM Service Started"
echo "=========================================="
echo ""
echo "Service Status:"
echo "  - vLLM (LLM):      http://localhost:8000  (PID: $VLLM_PID)"
echo ""
echo "Process IDs saved to:"
echo "  echo $VLLM_PID > logs/vllm.pid"
echo ""

# ä¿å­˜ PID
mkdir -p logs
echo $VLLM_PID > logs/vllm.pid

echo "To stop services, run:"
echo "  ./scripts/stop_local_services.sh"
echo ""
echo "To check logs:"
echo "  tail -f logs/vllm.log"
echo ""
echo "Note: Embedding service is using cloud API (not deployed locally)"
echo ""
