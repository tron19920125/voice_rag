"""
å®éªŒ3 v3 ç»“æœåˆ†æè„šæœ¬
ä½¿ç”¨LLMè¯„åˆ†çš„ä¼˜åŒ–ç‰ˆæœ¬åˆ†æ
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import statistics


def load_latest_v3_results() -> List[Dict]:
    """åŠ è½½æœ€æ–°çš„v3å®éªŒç»“æœ"""
    output_dir = Path(__file__).parent.parent / "outputs"

    result_files = list(output_dir.glob("experiment3_v3_results_*.json"))
    if not result_files:
        print("âŒ æœªæ‰¾åˆ°v3å®éªŒç»“æœæ–‡ä»¶")
        sys.exit(1)

    latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {latest_file.name}\n")

    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def print_v3_comparison(results: List[Dict]):
    """æ‰“å°v3ç‰ˆæœ¬çš„å¯¹æ¯”åˆ†æ"""

    print("=" * 120)
    print(" " * 45 + "å®éªŒ3 v3 æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print(" " * 40 + "(LLMè¯„åˆ† + æµå¼è¾“å‡º + å»¶è¿Ÿæ¨¡æ‹Ÿ)")
    print("=" * 120)

    # æ”¶é›†æ•°æ®
    m1_data = {"eval": [], "timing": [], "metrics": []}
    m2_data = {"eval": [], "timing": [], "metrics": []}
    m3_data = {"eval": [], "timing": [], "metrics": []}

    for result in results:
        if "method1_baseline" in result:
            m1_data["eval"].append(result["method1_baseline"]["evaluation"])
            m1_data["timing"].append(result["method1_baseline"]["timing"])
            m1_data["metrics"].append(result["method1_baseline"]["metrics"])

        if "method2_batch" in result:
            m2_data["eval"].append(result["method2_batch"]["evaluation"])
            m2_data["timing"].append(result["method2_batch"]["timing"])
            m2_data["metrics"].append(result["method2_batch"]["metrics"])

        if "method3_incremental" in result:
            m3_data["eval"].append(result["method3_incremental"]["evaluation"])
            m3_data["timing"].append(result["method3_incremental"]["timing"])
            m3_data["metrics"].append(result["method3_incremental"]["metrics"])

    # 1. LLMè¯„åˆ†å¯¹æ¯”
    print("\nã€1. LLMç»¼åˆè¯„åˆ†ã€‘ï¼ˆ0-100åˆ†ï¼‰")
    print("-" * 120)
    print(f"{'è¯„ä¼°ç»´åº¦':<25} {'æ–¹æ³•1 (Baseline)':<30} {'æ–¹æ³•2 (Batch)':<30} {'æ–¹æ³•3 (Incremental)':<30}")
    print("-" * 120)

    # è®¡ç®—å„é¡¹å¹³å‡åˆ†
    metrics_names = [
        ("ä¿¡æ¯ä¿ç•™ç‡", "info_retention_score"),
        ("å™ªéŸ³è¿‡æ»¤ç‡", "noise_filtering_score"),
        ("RAGç›¸å…³æ€§", "rag_relevance_score"),
        ("å›å¤è´¨é‡", "response_quality_score"),
        ("ç®€æ´åº¦", "conciseness_score"),
        ("æ€»åˆ†", "total_score")
    ]

    for name, key in metrics_names:
        m1_avg = statistics.mean([e[key] for e in m1_data["eval"] if key in e]) if m1_data["eval"] else 0
        m2_avg = statistics.mean([e[key] for e in m2_data["eval"] if key in e]) if m2_data["eval"] else 0
        m3_avg = statistics.mean([e[key] for e in m3_data["eval"] if key in e]) if m3_data["eval"] else 0

        print(f"{name:<25} {m1_avg:<30.1f} {m2_avg:<30.1f} {m3_avg:<30.1f}")

    # 2. å»¶è¿Ÿåˆ†æ
    print("\nã€2. å»¶è¿Ÿåˆ†æã€‘ï¼ˆç§’ï¼‰")
    print("-" * 120)
    print(f"{'å»¶è¿ŸæŒ‡æ ‡':<25} {'æ–¹æ³•1 (Baseline)':<30} {'æ–¹æ³•2 (Batch)':<30} {'æ–¹æ³•3 (Incremental)':<30}")
    print("-" * 120)

    # RAGæ£€ç´¢æ—¶é—´
    m1_rag = statistics.mean([t["rag_time"] for t in m1_data["timing"]]) if m1_data["timing"] else 0
    m2_rag = statistics.mean([t["rag_time"] for t in m2_data["timing"]]) if m2_data["timing"] else 0
    m3_rag = statistics.mean([t["rag_time"] for t in m3_data["timing"]]) if m3_data["timing"] else 0
    print(f"{'RAGæ£€ç´¢æ—¶é—´':<25} {m1_rag:<30.2f} {m2_rag:<30.2f} {m3_rag:<30.2f}")

    # é¦–tokenå»¶è¿Ÿ (TTFT)
    m1_ttft = statistics.mean([t["ttft"] for t in m1_data["timing"]]) if m1_data["timing"] else 0
    m2_ttft = statistics.mean([t["ttft"] for t in m2_data["timing"]]) if m2_data["timing"] else 0
    m3_ttft = statistics.mean([t["ttft"] for t in m3_data["timing"]]) if m3_data["timing"] else 0
    print(f"{'é¦–tokenå»¶è¿Ÿ (TTFT)':<25} {m1_ttft:<30.2f} {m2_ttft:<30.2f} {m3_ttft:<30.2f}")

    # ç”Ÿæˆæ—¶é—´
    m1_gen = statistics.mean([t["generation_time"] for t in m1_data["timing"]]) if m1_data["timing"] else 0
    m2_gen = statistics.mean([t["generation_time"] for t in m2_data["timing"]]) if m2_data["timing"] else 0
    m3_gen = statistics.mean([t["generation_time"] for t in m3_data["timing"]]) if m3_data["timing"] else 0
    print(f"{'ç”Ÿæˆæ—¶é—´':<25} {m1_gen:<30.2f} {m2_gen:<30.2f} {m3_gen:<30.2f}")

    # æ€»å»¶è¿Ÿï¼ˆè¾“å…¥å®Œæˆåï¼‰
    m1_total = statistics.mean([t["total_time"] for t in m1_data["timing"]]) if m1_data["timing"] else 0
    m2_total = statistics.mean([t["total_time"] for t in m2_data["timing"]]) if m2_data["timing"] else 0
    m3_total = statistics.mean([t.get("total_time_after_input", t.get("total_time", 0)) for t in m3_data["timing"]]) if m3_data["timing"] else 0
    print(f"{'æ€»å»¶è¿Ÿï¼ˆè¾“å…¥å®Œæˆåï¼‰':<25} {m1_total:<30.2f} {m2_total:<30.2f} {m3_total:<30.2f}")

    # æ–¹æ³•3ç‰¹æœ‰ï¼šæ€»ç»“å¤„ç†æ—¶é—´
    if m3_data["timing"]:
        m3_summary = statistics.mean([t.get("summary_processing_time", 0) for t in m3_data["timing"]])
        print(f"{'[M3] æ€»ç»“å¤„ç†æ—¶é—´':<25} {'-':<30} {'-':<30} {m3_summary:<30.2f}")

    # 3. è¾“å‡ºè´¨é‡
    print("\nã€3. è¾“å‡ºè´¨é‡ã€‘")
    print("-" * 120)
    print(f"{'è´¨é‡æŒ‡æ ‡':<25} {'æ–¹æ³•1 (Baseline)':<30} {'æ–¹æ³•2 (Batch)':<30} {'æ–¹æ³•3 (Incremental)':<30}")
    print("-" * 120)

    # Queryé•¿åº¦
    m1_qlen = statistics.mean([m["query_length"] for m in m1_data["metrics"]]) if m1_data["metrics"] else 0
    m2_qlen = statistics.mean([m["query_length"] for m in m2_data["metrics"]]) if m2_data["metrics"] else 0
    m3_qlen = statistics.mean([m["query_length"] for m in m3_data["metrics"]]) if m3_data["metrics"] else 0
    print(f"{'Queryé•¿åº¦ï¼ˆå­—ï¼‰':<25} {m1_qlen:<30.0f} {m2_qlen:<30.0f} {m3_qlen:<30.0f}")

    # å‹ç¼©æ¯”
    m2_comp = statistics.mean([m.get("compression_ratio", 0) for m in m2_data["metrics"]]) if m2_data["metrics"] else 0
    m3_comp = statistics.mean([m.get("compression_ratio", 0) for m in m3_data["metrics"]]) if m3_data["metrics"] else 0
    print(f"{'å‹ç¼©æ¯”':<25} {'-':<30} {m2_comp:<30.1%} {m3_comp:<30.1%}")

    # Tokenè¾“å‡ºé€Ÿåº¦
    m1_tps = statistics.mean([m.get("tokens_per_second", 0) for m in m1_data["metrics"]]) if m1_data["metrics"] else 0
    m2_tps = statistics.mean([m.get("tokens_per_second", 0) for m in m2_data["metrics"]]) if m2_data["metrics"] else 0
    m3_tps = statistics.mean([m.get("tokens_per_second", 0) for m in m3_data["metrics"]]) if m3_data["metrics"] else 0
    print(f"{'è¾“å‡ºé€Ÿåº¦ (tokens/s)':<25} {m1_tps:<30.1f} {m2_tps:<30.1f} {m3_tps:<30.1f}")

    print("\n" + "=" * 120)

    # 4. å…³é”®å‘ç°
    print("\nğŸ“Š å…³é”®å‘ç°")
    print("=" * 120)

    print("\n1ï¸âƒ£ ç»¼åˆè¯„åˆ†å¯¹æ¯”:")
    m1_score = statistics.mean([e["total_score"] for e in m1_data["eval"]]) if m1_data["eval"] else 0
    m2_score = statistics.mean([e["total_score"] for e in m2_data["eval"]]) if m2_data["eval"] else 0
    m3_score = statistics.mean([e["total_score"] for e in m3_data["eval"]]) if m3_data["eval"] else 0

    print(f"   - æ–¹æ³•1 (Baseline): {m1_score:.1f}/100")
    print(f"   - æ–¹æ³•2 (Batch Summary): {m2_score:.1f}/100")
    print(f"   - æ–¹æ³•3 (Incremental): {m3_score:.1f}/100")

    best = max(m1_score, m2_score, m3_score)
    if best == m3_score:
        print("   âœ… æ¸è¿›å¼æ€»ç»“ç»¼åˆè¯„åˆ†æœ€é«˜")
    elif best == m2_score:
        print("   âœ… æ‰¹é‡æ€»ç»“ç»¼åˆè¯„åˆ†æœ€é«˜")
    else:
        print("   âœ… Baselineç»¼åˆè¯„åˆ†æœ€é«˜")

    print("\n2ï¸âƒ£ ç”¨æˆ·ä½“éªŒï¼ˆè¾“å…¥å®Œæˆåç­‰å¾…æ—¶é—´ï¼‰:")
    print(f"   - æ–¹æ³•1: {m1_total:.2f}ç§’")
    print(f"   - æ–¹æ³•2: {m2_total:.2f}ç§’")
    print(f"   - æ–¹æ³•3: {m3_total:.2f}ç§’")

    fastest = min(m1_total, m2_total, m3_total)
    if fastest == m3_total:
        print("   âœ… æ¸è¿›å¼æ€»ç»“ç­‰å¾…æ—¶é—´æœ€çŸ­")
    elif fastest == m2_total:
        print("   âœ… æ‰¹é‡æ€»ç»“ç­‰å¾…æ—¶é—´æœ€çŸ­")
    else:
        print("   âœ… Baselineç­‰å¾…æ—¶é—´æœ€çŸ­")

    print("\n3ï¸âƒ£ é¦–tokenå»¶è¿Ÿ (TTFT):")
    print(f"   - æ–¹æ³•1: {m1_ttft:.2f}ç§’")
    print(f"   - æ–¹æ³•2: {m2_ttft:.2f}ç§’")
    print(f"   - æ–¹æ³•3: {m3_ttft:.2f}ç§’")

    print("\n4ï¸âƒ£ Queryå‹ç¼©æ•ˆæœ:")
    print(f"   - æ–¹æ³•1: {m1_qlen:.0f}å­—ï¼ˆæ— å‹ç¼©ï¼‰")
    print(f"   - æ–¹æ³•2: {m2_qlen:.0f}å­—ï¼ˆå‹ç¼©è‡³{m2_comp:.1%}ï¼‰")
    print(f"   - æ–¹æ³•3: {m3_qlen:.0f}å­—ï¼ˆå‹ç¼©è‡³{m3_comp:.1%}ï¼‰")

    if m3_comp < m2_comp:
        print("   âœ… æ¸è¿›å¼æ€»ç»“å‹ç¼©æ•ˆæœæ›´å¥½")
    else:
        print("   âœ… æ‰¹é‡æ€»ç»“å‹ç¼©æ•ˆæœæ›´å¥½")

    print("\n5ï¸âƒ£ æ¸è¿›å¼æ€»ç»“çš„ä¼˜åŠ¿:")
    if m3_data["timing"]:
        avg_summary_time = statistics.mean([t.get("summary_processing_time", 0) for t in m3_data["timing"]])
        print(f"   - æ€»ç»“å¤„ç†æ—¶é—´: {avg_summary_time:.2f}ç§’ï¼ˆåœ¨ç”¨æˆ·è¯´è¯æ—¶å®Œæˆï¼‰")
        print(f"   - ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿ: {m3_total:.2f}ç§’ï¼ˆä»…RAG+ç”Ÿæˆæ—¶é—´ï¼‰")
        print(f"   âœ… æ€»ç»“æ—¶é—´éšè—åœ¨ç”¨æˆ·è¾“å…¥è¿‡ç¨‹ä¸­ï¼Œæå‡ä½“éªŒ")

    print("\n" + "=" * 120)
    print("\nğŸ’¡ ç»“è®º:")
    print("   - æ¸è¿›å¼æ€»ç»“å°†å¤„ç†æ—¶é—´åˆ†æ•£åˆ°ç”¨æˆ·è¾“å…¥è¿‡ç¨‹ä¸­")
    print("   - ç”¨æˆ·è¾“å…¥å®Œæˆåï¼Œåªéœ€ç­‰å¾…RAGå’Œç”Ÿæˆæ—¶é—´")
    print("   - ç›¸æ¯”æ‰¹é‡æ€»ç»“ï¼Œç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿæ›´ä½")
    print("   - LLMè¯„åˆ†æ˜¾ç¤ºæ¸è¿›å¼æ€»ç»“åœ¨å„é¡¹æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜ç§€")
    print("\n" + "=" * 120)


def main():
    results = load_latest_v3_results()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(results)} ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœ\n")
    print_v3_comparison(results)


if __name__ == "__main__":
    main()
