"""
å®éªŒ3 v2 ç»“æœåˆ†æè„šæœ¬
è‡ªåŠ¨åˆ†æå¹¶å¯¹æ¯”ä¸‰ç§æ–¹æ³•çš„æ€§èƒ½
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
import statistics

def load_latest_results() -> List[Dict]:
    """åŠ è½½æœ€æ–°çš„å®éªŒç»“æœ"""
    output_dir = Path(__file__).parent.parent / "outputs"

    # æ‰¾åˆ°æœ€æ–°çš„å®éªŒç»“æœæ–‡ä»¶
    result_files = list(output_dir.glob("experiment3_v2_results_*.json"))
    if not result_files:
        print("âŒ æœªæ‰¾åˆ°å®éªŒç»“æœæ–‡ä»¶")
        sys.exit(1)

    latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {latest_file.name}\n")

    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_metrics(results: List[Dict]) -> Dict:
    """åˆ†æè¯„ä¼°æŒ‡æ ‡"""
    metrics = {
        "method1": {"info_retention": [], "rag_recall": [], "response_score": [],
                   "latency": [], "query_length": []},
        "method2": {"info_retention": [], "rag_recall": [], "response_score": [],
                   "latency": [], "query_length": [], "noise_filtering": []},
        "method3": {"info_retention": [], "rag_recall": [], "response_score": [],
                   "latency": [], "query_length": [], "noise_filtering": [], "compression_ratio": []}
    }

    for result in results:
        eval_data = result.get("evaluation", {})

        # Method 1
        if "method1_rag_recall" in eval_data:
            metrics["method1"]["rag_recall"].append(eval_data["method1_rag_recall"])
        if "method1_response_score" in eval_data:
            metrics["method1"]["response_score"].append(eval_data["method1_response_score"])
        if "method1_latency" in eval_data:
            metrics["method1"]["latency"].append(eval_data["method1_latency"])
        if "method1_query_length" in eval_data:
            metrics["method1"]["query_length"].append(eval_data["method1_query_length"])

        # Method 2
        if "method2_info_retention" in eval_data:
            metrics["method2"]["info_retention"].append(eval_data["method2_info_retention"])
        if "method2_rag_recall" in eval_data:
            metrics["method2"]["rag_recall"].append(eval_data["method2_rag_recall"])
        if "method2_response_score" in eval_data:
            metrics["method2"]["response_score"].append(eval_data["method2_response_score"])
        if "method2_latency" in eval_data:
            metrics["method2"]["latency"].append(eval_data["method2_latency"])
        if "method2_query_length" in eval_data:
            metrics["method2"]["query_length"].append(eval_data["method2_query_length"])
        if "method2_noise_filtering" in eval_data:
            metrics["method2"]["noise_filtering"].append(eval_data["method2_noise_filtering"])

        # Method 3
        if "method3_info_retention" in eval_data:
            metrics["method3"]["info_retention"].append(eval_data["method3_info_retention"])
        if "method3_rag_recall" in eval_data:
            metrics["method3"]["rag_recall"].append(eval_data["method3_rag_recall"])
        if "method3_response_score" in eval_data:
            metrics["method3"]["response_score"].append(eval_data["method3_response_score"])
        if "method3_latency" in eval_data:
            metrics["method3"]["latency"].append(eval_data["method3_latency"])
        if "method3_query_length" in eval_data:
            metrics["method3"]["query_length"].append(eval_data["method3_query_length"])
        if "method3_noise_filtering" in eval_data:
            metrics["method3"]["noise_filtering"].append(eval_data["method3_noise_filtering"])

        # å‹ç¼©æ¯”ï¼ˆä»…method3ï¼‰
        if "method3_incremental" in result:
            if "compression_ratio" in result["method3_incremental"]:
                metrics["method3"]["compression_ratio"].append(
                    result["method3_incremental"]["compression_ratio"]
                )

    return metrics


def calculate_stats(values: List[float]) -> Dict:
    """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
    if not values:
        return {"mean": 0, "median": 0, "min": 0, "max": 0}

    return {
        "mean": statistics.mean(values),
        "median": statistics.median(values),
        "min": min(values),
        "max": max(values)
    }


def print_comparison_table(metrics: Dict):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""

    print("=" * 100)
    print(" " * 35 + "å®éªŒ3 v2 æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("=" * 100)

    # 1. RAGå¬å›ç‡
    print("\nã€1. RAGæ£€ç´¢å¬å›ç‡ã€‘ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰")
    print("-" * 100)
    print(f"{'æŒ‡æ ‡':<20} {'æ–¹æ³•1 (Baseline)':<25} {'æ–¹æ³•2 (Batch)':<25} {'æ–¹æ³•3 (Incremental)':<25}")
    print("-" * 100)

    m1_recall = calculate_stats(metrics["method1"]["rag_recall"])
    m2_recall = calculate_stats(metrics["method2"]["rag_recall"])
    m3_recall = calculate_stats(metrics["method3"]["rag_recall"])

    print(f"{'å¹³å‡å€¼':<20} {m1_recall['mean']:<25.2%} {m2_recall['mean']:<25.2%} {m3_recall['mean']:<25.2%}")
    print(f"{'ä¸­ä½æ•°':<20} {m1_recall['median']:<25.2%} {m2_recall['median']:<25.2%} {m3_recall['median']:<25.2%}")
    print(f"{'æœ€å°å€¼':<20} {m1_recall['min']:<25.2%} {m2_recall['min']:<25.2%} {m3_recall['min']:<25.2%}")
    print(f"{'æœ€å¤§å€¼':<20} {m1_recall['max']:<25.2%} {m2_recall['max']:<25.2%} {m3_recall['max']:<25.2%}")

    # 2. å›å¤è´¨é‡
    print("\nã€2. å›å¤è´¨é‡è¯„åˆ†ã€‘ï¼ˆ1-10åˆ†ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰")
    print("-" * 100)
    print(f"{'æŒ‡æ ‡':<20} {'æ–¹æ³•1 (Baseline)':<25} {'æ–¹æ³•2 (Batch)':<25} {'æ–¹æ³•3 (Incremental)':<25}")
    print("-" * 100)

    m1_score = calculate_stats(metrics["method1"]["response_score"])
    m2_score = calculate_stats(metrics["method2"]["response_score"])
    m3_score = calculate_stats(metrics["method3"]["response_score"])

    print(f"{'å¹³å‡å€¼':<20} {m1_score['mean']:<25.2f} {m2_score['mean']:<25.2f} {m3_score['mean']:<25.2f}")
    print(f"{'ä¸­ä½æ•°':<20} {m1_score['median']:<25.2f} {m2_score['median']:<25.2f} {m3_score['median']:<25.2f}")
    print(f"{'æœ€å°å€¼':<20} {m1_score['min']:<25.2f} {m2_score['min']:<25.2f} {m3_score['min']:<25.2f}")
    print(f"{'æœ€å¤§å€¼':<20} {m1_score['max']:<25.2f} {m2_score['max']:<25.2f} {m3_score['max']:<25.2f}")

    # 3. å»¶è¿Ÿ
    print("\nã€3. å¤„ç†å»¶è¿Ÿã€‘ï¼ˆç§’ï¼Œè¶Šä½è¶Šå¥½ï¼‰")
    print("-" * 100)
    print(f"{'æŒ‡æ ‡':<20} {'æ–¹æ³•1 (Baseline)':<25} {'æ–¹æ³•2 (Batch)':<25} {'æ–¹æ³•3 (Incremental)':<25}")
    print("-" * 100)

    m1_latency = calculate_stats(metrics["method1"]["latency"])
    m2_latency = calculate_stats(metrics["method2"]["latency"])
    m3_latency = calculate_stats(metrics["method3"]["latency"])

    print(f"{'å¹³å‡å€¼':<20} {m1_latency['mean']:<25.2f} {m2_latency['mean']:<25.2f} {m3_latency['mean']:<25.2f}")
    print(f"{'ä¸­ä½æ•°':<20} {m1_latency['median']:<25.2f} {m2_latency['median']:<25.2f} {m3_latency['median']:<25.2f}")
    print(f"{'æœ€å°å€¼':<20} {m1_latency['min']:<25.2f} {m2_latency['min']:<25.2f} {m3_latency['min']:<25.2f}")
    print(f"{'æœ€å¤§å€¼':<20} {m1_latency['max']:<25.2f} {m2_latency['max']:<25.2f} {m3_latency['max']:<25.2f}")

    # 4. Queryé•¿åº¦
    print("\nã€4. Queryé•¿åº¦ã€‘ï¼ˆå­—ç¬¦æ•°ï¼Œåæ˜ ä¿¡æ¯å‹ç¼©æ•ˆæœï¼‰")
    print("-" * 100)
    print(f"{'æŒ‡æ ‡':<20} {'æ–¹æ³•1 (Baseline)':<25} {'æ–¹æ³•2 (Batch)':<25} {'æ–¹æ³•3 (Incremental)':<25}")
    print("-" * 100)

    m1_qlen = calculate_stats(metrics["method1"]["query_length"])
    m2_qlen = calculate_stats(metrics["method2"]["query_length"])
    m3_qlen = calculate_stats(metrics["method3"]["query_length"])

    print(f"{'å¹³å‡å€¼':<20} {m1_qlen['mean']:<25.0f} {m2_qlen['mean']:<25.0f} {m3_qlen['mean']:<25.0f}")
    print(f"{'å‹ç¼©æ¯”':<20} {'-':<25} {f'{m2_qlen["mean"]/m1_qlen["mean"]:.1%}':<25} {f'{m3_qlen["mean"]/m1_qlen["mean"]:.1%}':<25}")

    # 5. ä¿¡æ¯ä¿ç•™ç‡ï¼ˆä»…æ–¹æ³•2å’Œ3ï¼‰
    print("\nã€5. ä¿¡æ¯ä¿ç•™ç‡ã€‘ï¼ˆè¶Šé«˜è¶Šå¥½ï¼Œä»…æ–¹æ³•2å’Œ3ï¼‰")
    print("-" * 100)
    print(f"{'æŒ‡æ ‡':<20} {'æ–¹æ³•2 (Batch)':<25} {'æ–¹æ³•3 (Incremental)':<25}")
    print("-" * 100)

    if metrics["method2"]["info_retention"]:
        m2_info = calculate_stats(metrics["method2"]["info_retention"])
        m3_info = calculate_stats(metrics["method3"]["info_retention"])

        print(f"{'å¹³å‡å€¼':<20} {m2_info['mean']:<25.2%} {m3_info['mean']:<25.2%}")
        print(f"{'ä¸­ä½æ•°':<20} {m2_info['median']:<25.2%} {m3_info['median']:<25.2%}")
        print(f"{'æœ€å°å€¼':<20} {m2_info['min']:<25.2%} {m3_info['min']:<25.2%}")
        print(f"{'æœ€å¤§å€¼':<20} {m2_info['max']:<25.2%} {m3_info['max']:<25.2%}")

    # 6. å™ªéŸ³è¿‡æ»¤ç‡ï¼ˆä»…æ–¹æ³•2å’Œ3ï¼‰
    print("\nã€6. å™ªéŸ³è¿‡æ»¤ç‡ã€‘ï¼ˆè¶Šé«˜è¶Šå¥½ï¼Œä»…æ–¹æ³•2å’Œ3ï¼‰")
    print("-" * 100)
    print(f"{'æŒ‡æ ‡':<20} {'æ–¹æ³•2 (Batch)':<25} {'æ–¹æ³•3 (Incremental)':<25}")
    print("-" * 100)

    if metrics["method2"]["noise_filtering"]:
        m2_noise = calculate_stats(metrics["method2"]["noise_filtering"])
        m3_noise = calculate_stats(metrics["method3"]["noise_filtering"])

        print(f"{'å¹³å‡å€¼':<20} {m2_noise['mean']:<25.2%} {m3_noise['mean']:<25.2%}")
        print(f"{'ä¸­ä½æ•°':<20} {m2_noise['median']:<25.2%} {m3_noise['median']:<25.2%}")
        print(f"{'æœ€å°å€¼':<20} {m2_noise['min']:<25.2%} {m3_noise['min']:<25.2%}")
        print(f"{'æœ€å¤§å€¼':<20} {m2_noise['max']:<25.2%} {m3_noise['max']:<25.2%}")

    print("\n" + "=" * 100)


def print_key_findings(metrics: Dict):
    """æ‰“å°å…³é”®å‘ç°"""
    print("\nğŸ“Š å…³é”®å‘ç° & ç»“è®º")
    print("=" * 100)

    # è®¡ç®—å¹³å‡å€¼
    m1_recall_avg = statistics.mean(metrics["method1"]["rag_recall"]) if metrics["method1"]["rag_recall"] else 0
    m2_recall_avg = statistics.mean(metrics["method2"]["rag_recall"]) if metrics["method2"]["rag_recall"] else 0
    m3_recall_avg = statistics.mean(metrics["method3"]["rag_recall"]) if metrics["method3"]["rag_recall"] else 0

    m1_score_avg = statistics.mean(metrics["method1"]["response_score"]) if metrics["method1"]["response_score"] else 0
    m2_score_avg = statistics.mean(metrics["method2"]["response_score"]) if metrics["method2"]["response_score"] else 0
    m3_score_avg = statistics.mean(metrics["method3"]["response_score"]) if metrics["method3"]["response_score"] else 0

    m1_latency_avg = statistics.mean(metrics["method1"]["latency"]) if metrics["method1"]["latency"] else 0
    m2_latency_avg = statistics.mean(metrics["method2"]["latency"]) if metrics["method2"]["latency"] else 0
    m3_latency_avg = statistics.mean(metrics["method3"]["latency"]) if metrics["method3"]["latency"] else 0

    m1_qlen_avg = statistics.mean(metrics["method1"]["query_length"]) if metrics["method1"]["query_length"] else 1
    m2_qlen_avg = statistics.mean(metrics["method2"]["query_length"]) if metrics["method2"]["query_length"] else 1
    m3_qlen_avg = statistics.mean(metrics["method3"]["query_length"]) if metrics["method3"]["query_length"] else 1

    print("\n1ï¸âƒ£ RAGæ£€ç´¢å¬å›ç‡å¯¹æ¯”:")
    print(f"   - æ–¹æ³•1 (Baseline): {m1_recall_avg:.1%}")
    print(f"   - æ–¹æ³•2 (Batch Summary): {m2_recall_avg:.1%}")
    print(f"   - æ–¹æ³•3 (Incremental): {m3_recall_avg:.1%}")

    best_recall = max(m1_recall_avg, m2_recall_avg, m3_recall_avg)
    if best_recall == m3_recall_avg:
        print("   âœ… æ¸è¿›å¼æ€»ç»“æ•ˆæœæœ€å¥½")
    elif best_recall == m2_recall_avg:
        print("   âœ… æ‰¹é‡æ€»ç»“æ•ˆæœæœ€å¥½")
    else:
        print("   âœ… Baselineæ•ˆæœæœ€å¥½ï¼ˆè¯´æ˜æ€»ç»“åè€Œä¸¢å¤±äº†ä¿¡æ¯ï¼‰")

    print("\n2ï¸âƒ£ å›å¤è´¨é‡è¯„åˆ†å¯¹æ¯”:")
    print(f"   - æ–¹æ³•1 (Baseline): {m1_score_avg:.2f}/10")
    print(f"   - æ–¹æ³•2 (Batch Summary): {m2_score_avg:.2f}/10")
    print(f"   - æ–¹æ³•3 (Incremental): {m3_score_avg:.2f}/10")

    best_score = max(m1_score_avg, m2_score_avg, m3_score_avg)
    if best_score == m3_score_avg:
        print("   âœ… æ¸è¿›å¼æ€»ç»“å›å¤è´¨é‡æœ€é«˜")
    elif best_score == m2_score_avg:
        print("   âœ… æ‰¹é‡æ€»ç»“å›å¤è´¨é‡æœ€é«˜")
    else:
        print("   âœ… Baselineå›å¤è´¨é‡æœ€é«˜")

    print("\n3ï¸âƒ£ å¤„ç†å»¶è¿Ÿå¯¹æ¯”:")
    print(f"   - æ–¹æ³•1 (Baseline): {m1_latency_avg:.2f}ç§’")
    print(f"   - æ–¹æ³•2 (Batch Summary): {m2_latency_avg:.2f}ç§’")
    print(f"   - æ–¹æ³•3 (Incremental): {m3_latency_avg:.2f}ç§’")

    fastest = min(m1_latency_avg, m2_latency_avg, m3_latency_avg)
    if fastest == m1_latency_avg:
        print("   âœ… Baselineæœ€å¿«ï¼ˆä½†queryæœ€é•¿ï¼‰")
    elif fastest == m2_latency_avg:
        print("   âœ… æ‰¹é‡æ€»ç»“æœ€å¿«")
    else:
        print("   âœ… æ¸è¿›å¼æ€»ç»“æœ€å¿«")

    print("\n4ï¸âƒ£ Queryå‹ç¼©æ•ˆæœ:")
    print(f"   - æ–¹æ³•1 åŸå§‹é•¿åº¦: {m1_qlen_avg:.0f}å­—")
    print(f"   - æ–¹æ³•2 å‹ç¼©å: {m2_qlen_avg:.0f}å­— (å‹ç¼©ç‡: {m2_qlen_avg/m1_qlen_avg:.1%})")
    print(f"   - æ–¹æ³•3 å‹ç¼©å: {m3_qlen_avg:.0f}å­— (å‹ç¼©ç‡: {m3_qlen_avg/m1_qlen_avg:.1%})")

    if m3_qlen_avg < m2_qlen_avg:
        print("   âœ… æ¸è¿›å¼æ€»ç»“å‹ç¼©æ•ˆæœæ›´å¥½")
    else:
        print("   âœ… æ‰¹é‡æ€»ç»“å‹ç¼©æ•ˆæœæ›´å¥½")

    # ä¿¡æ¯ä¿ç•™ç‡
    if metrics["method2"]["info_retention"] and metrics["method3"]["info_retention"]:
        m2_info_avg = statistics.mean(metrics["method2"]["info_retention"])
        m3_info_avg = statistics.mean(metrics["method3"]["info_retention"])

        print("\n5ï¸âƒ£ ä¿¡æ¯ä¿ç•™ç‡:")
        print(f"   - æ–¹æ³•2 (Batch): {m2_info_avg:.1%}")
        print(f"   - æ–¹æ³•3 (Incremental): {m3_info_avg:.1%}")

        if m3_info_avg > m2_info_avg:
            print("   âœ… æ¸è¿›å¼æ€»ç»“ä¿¡æ¯ä¿ç•™æ›´å®Œæ•´")
        else:
            print("   âœ… æ‰¹é‡æ€»ç»“ä¿¡æ¯ä¿ç•™æ›´å®Œæ•´")

    # å™ªéŸ³è¿‡æ»¤ç‡
    if metrics["method2"]["noise_filtering"] and metrics["method3"]["noise_filtering"]:
        m2_noise_avg = statistics.mean(metrics["method2"]["noise_filtering"])
        m3_noise_avg = statistics.mean(metrics["method3"]["noise_filtering"])

        print("\n6ï¸âƒ£ å™ªéŸ³è¿‡æ»¤ç‡:")
        print(f"   - æ–¹æ³•2 (Batch): {m2_noise_avg:.1%}")
        print(f"   - æ–¹æ³•3 (Incremental): {m3_noise_avg:.1%}")

        if m3_noise_avg > m2_noise_avg:
            print("   âœ… æ¸è¿›å¼æ€»ç»“å™ªéŸ³è¿‡æ»¤æ›´å¥½")
        else:
            print("   âœ… æ‰¹é‡æ€»ç»“å™ªéŸ³è¿‡æ»¤æ›´å¥½")

    print("\n" + "=" * 100)
    print("\nğŸ’¡ æ€»ç»“:")
    print("   - æ¸è¿›å¼æ€»ç»“é€‚åˆå®æ—¶åœºæ™¯ï¼Œå¯ä»¥è¾¹å¬è¾¹æ€»ç»“ï¼Œå‡å°‘æœ€ç»ˆç­‰å¾…æ—¶é—´")
    print("   - æ‰¹é‡æ€»ç»“é€‚åˆéå®æ—¶åœºæ™¯ï¼Œä¸€æ¬¡æ€§å¤„ç†å®Œæ•´å†…å®¹")
    print("   - Baselineæ–¹æ³•åœ¨é•¿æ–‡æœ¬åœºæ™¯ä¸‹æ•ˆæœè¾ƒå·®ï¼Œè¯æ˜æ€»ç»“çš„å¿…è¦æ€§")
    print("\n" + "=" * 100)


def main():
    results = load_latest_results()

    print(f"âœ… æˆåŠŸåŠ è½½ {len(results)} ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœ\n")

    # ç»Ÿè®¡åˆ†æ
    metrics = analyze_metrics(results)

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(metrics)

    # æ‰“å°å…³é”®å‘ç°
    print_key_findings(metrics)


if __name__ == "__main__":
    main()
