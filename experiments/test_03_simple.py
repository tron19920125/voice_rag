#!/usr/bin/env python3
"""
Experiment 3: Local vLLM vs Cloud API Performance Comparison
æœ¬åœ° vLLM vs äº‘ç«¯ API æ€§èƒ½å¯¹æ¯”

ç›®æ ‡ï¼š
1. å¯¹æ¯”æœ¬åœ°éƒ¨ç½² LLM (vLLM + Qwen3-8B) vs äº‘ç«¯ API çš„æ€§èƒ½
2. æµ‹è¯•å»¶è¿Ÿã€ååé‡ç­‰å…³é”®æŒ‡æ ‡
3. éªŒè¯æœ¬åœ°éƒ¨ç½²çš„å¯è¡Œæ€§

é…ç½®ï¼š
- æœ¬åœ°: vLLM 0.11.0 + Qwen3-8B on V100 x2 (tensor parallelism)
- äº‘ç«¯: é€šä¹‰åƒé—® API (qwen3-8b)
- æ€è€ƒæ¨¡å¼: ä¸¤è¾¹éƒ½ç¦ç”¨ (enable_thinking=false)

æµ‹è¯•æ–¹æ³•ï¼š
- ä½¿ç”¨ 3 ä¸ªç®€å•é—®é¢˜æµ‹è¯•åŸºç¡€æ€§èƒ½
- æµ‹é‡ç«¯åˆ°ç«¯å»¶è¿Ÿå’Œ token ä½¿ç”¨é‡
- å¯¹æ¯”å¹³å‡æ€§èƒ½æŒ‡æ ‡
"""

import json
import time
from datetime import datetime
from openai import OpenAI
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æµ‹è¯•é—®é¢˜
test_questions = [
    "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    "1+1ç­‰äºå‡ ï¼Ÿ",
    "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚",
]

def test_model(client, model, name, is_cloud=False):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    print(f"\n{'='*60}")
    print(f"Testing {name}")
    print(f"{'='*60}")

    results = []

    for i, question in enumerate(test_questions, 1):
        print(f"\n[{i}/{len(test_questions)}] é—®é¢˜: {question}")

        try:
            start_time = time.time()

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": "You are a helpful assistant. Answer directly and concisely."},
                {"role": "user", "content": question}
            ]

            # äº‘ç«¯ API ä½¿ç”¨ extra_body ç¦ç”¨æ€è€ƒæ¨¡å¼
            if is_cloud:
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=100,
                    temperature=0.1,
                    extra_body={"enable_thinking": False}
                )
            else:
                # æœ¬åœ° vLLM ä½¿ç”¨ chat_template_kwargs ç¦ç”¨æ€è€ƒæ¨¡å¼
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=100,
                    temperature=0.1,
                    extra_body={"chat_template_kwargs": {"enable_thinking": False}}
                )

            latency = time.time() - start_time
            answer = response.choices[0].message.content

            result = {
                "question": question,
                "answer": answer[:200] + "..." if len(answer) > 200 else answer,
                "latency": latency,
                "tokens": {
                    "prompt": response.usage.prompt_tokens,
                    "completion": response.usage.completion_tokens,
                    "total": response.usage.total_tokens
                }
            }

            results.append(result)

            print(f"âœ… å»¶è¿Ÿ: {latency:.2f}s")
            print(f"ğŸ“Š Tokens: {response.usage.total_tokens}")
            print(f"ğŸ“ å›ç­”: {answer[:100]}...")

        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            results.append({
                "question": question,
                "error": str(e)
            })

    return results


def main():
    print("\n" + "="*80)
    print("Experiment 3: Local vLLM vs Cloud API Performance Test")
    print("="*80)

    all_results = {
        "timestamp": datetime.now().isoformat(),
        "cloud": None,
        "local": None
    }

    # 1. æµ‹è¯•äº‘ç«¯ API
    print("\n\n" + "="*80)
    print("Part 1: Testing Cloud API")
    print("="*80)

    try:
        cloud_client = OpenAI(
            api_key=os.getenv("QWEN_TOKEN"),
            base_url=os.getenv("QWEN_API_BASE")
        )
        cloud_model = os.getenv("QWEN_MODEL")

        cloud_results = test_model(cloud_client, cloud_model, "Cloud API", is_cloud=True)
        all_results["cloud"] = cloud_results

    except Exception as e:
        print(f"\nâŒ äº‘ç«¯ API æµ‹è¯•å¤±è´¥: {e}")

    # 2. æµ‹è¯•æœ¬åœ° vLLM
    print("\n\n" + "="*80)
    print("Part 2: Testing Local vLLM")
    print("="*80)

    try:
        local_client = OpenAI(
            api_key="EMPTY",
            base_url="http://localhost:8000/v1"
        )
        local_model = "Qwen/Qwen3-8B"

        local_results = test_model(local_client, local_model, "Local vLLM", is_cloud=False)
        all_results["local"] = local_results

    except Exception as e:
        print(f"\nâŒ æœ¬åœ° vLLM æµ‹è¯•å¤±è´¥: {e}")

    # 3. å¯¹æ¯”åˆ†æ
    print("\n\n" + "="*80)
    print("ğŸ“Š Performance Comparison")
    print("="*80)

    if all_results["cloud"] and all_results["local"]:
        cloud_valid = [r for r in all_results["cloud"] if "latency" in r]
        local_valid = [r for r in all_results["local"] if "latency" in r]

        if cloud_valid and local_valid:
            cloud_avg = sum(r["latency"] for r in cloud_valid) / len(cloud_valid)
            local_avg = sum(r["latency"] for r in local_valid) / len(local_valid)

            print(f"\nâ±ï¸  Average Latency:")
            print(f"   Cloud API:  {cloud_avg:.3f}s")
            print(f"   Local vLLM: {local_avg:.3f}s")
            print(f"   Speedup:    {cloud_avg / local_avg:.2f}x")

            cloud_tokens = sum(r["tokens"]["total"] for r in cloud_valid) / len(cloud_valid)
            local_tokens = sum(r["tokens"]["total"] for r in local_valid) / len(local_valid)

            print(f"\nğŸ“Š Average Tokens:")
            print(f"   Cloud API:  {cloud_tokens:.0f}")
            print(f"   Local vLLM: {local_tokens:.0f}")

    # 4. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"outputs/experiment3_simple_{timestamp}.json"

    os.makedirs("outputs", exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\n\nğŸ’¾ Results saved to: {output_file}")
    print()


if __name__ == "__main__":
    main()
