# 实验3：长时间语音输入的分段总结与RAG处理

**实验日期**: 2025-12-20
**目标**: 解决用户在长时间语音输入后如何更加准确回复的问题

---

## 一、问题背景

### 1.1 业务场景

在实际的语音交互场景中，用户可能会进行长时间的语音输入（例如：
- 详细描述一个复杂的业务需求（2-5分钟）
- 讲述一段完整的故事或案例
- 提出多个相关的问题
- 进行长篇的产品咨询

### 1.2 核心挑战

**问题1：信息过载**
- 长语音转文本后可能有500-2000字
- 包含多个主题和意图
- 直接RAG检索效果不佳（query太长）

**问题2：检索精度下降**
- 长query包含过多信息
- 关键信息被淹没在细节中
- 向量检索难以捕捉多个核心要点

**问题3：响应延迟**
- 用户说完后期望快速回复
- 长文本的embedding和检索耗时

---

## 二、解决方案设计

### 2.1 核心思路

**分段总结 + 结构化提取 + RAG检索**

```
用户长语音
  ↓
[VAD切分] → 多个音频片段
  ↓
[ASR转文本] → 多个文本片段（流式输入）
  ↓
[分段总结] → 提取关键信息
  ↓
[意图识别] → 识别查询类型
  ↓
[RAG检索] → 基于总结后的query检索
  ↓
[LLM生成] → 生成完整回复
```

### 2.2 技术方案

#### 方案A：渐进式总结（Incremental Summarization）

**适用场景**: 实时交互，需要边听边理解

```python
# 流式处理
summary = ""
segments = []

for audio_chunk in stream:
    # 1. ASR转文本
    text = asr.transcribe(audio_chunk)
    segments.append(text)

    # 2. 渐进式更新总结
    if len(text) > 50:  # 足够的信息量
        summary = update_summary(summary, text)

    # 3. VAD检测到停顿
    if vad.is_pause():
        break

# 4. 完成后进行RAG
results = rag.query(summary)
response = llm.generate(summary, results)
```

**优点**:
- 实时性好，用户体验流畅
- 可以提前准备检索

**缺点**:
- 需要多次LLM调用（成本高）
- 早期片段可能理解不完整

#### 方案B：完整总结（Batch Summarization）⭐ 推荐

**适用场景**: 用户说完后再处理，追求准确性

```python
# 1. 收集完整输入
full_text = ""
while True:
    text = asr.transcribe(audio_chunk)
    full_text += text

    # VAD检测到完成
    if vad.is_complete():
        break

# 2. 一次性总结和结构化
structured_query = extract_key_info(full_text)
# 返回: {
#   "main_intent": "产品咨询",
#   "key_points": ["查询客户背景", "推荐产品", "预算约束"],
#   "entities": ["公司A", "金融行业"],
#   "concise_query": "查询公司A的背景并推荐适合金融行业的产品，预算50万"
# }

# 3. RAG检索
results = rag.query(structured_query["concise_query"])

# 4. 生成回复（带上原始文本和总结）
response = llm.generate(
    original_text=full_text,
    structured_query=structured_query,
    rag_results=results
)
```

**优点**:
- 只需一次LLM调用（成本低）
- 信息完整，理解准确
- 可以进行更深度的分析

**缺点**:
- 需要等待用户说完（延迟稍高）

---

## 三、实验设计

### 3.1 测试场景设计

为了模拟长时间语音输入，我们设计以下测试场景：

#### 场景1：复杂产品咨询（冗长版）
```
原始输入（200字）:
"你好，我想咨询一下你们公司的产品。我们是一家金融科技公司，最近在做数字化转型，
遇到了一些问题。我们现在有大约100人的团队，主要做供应链金融的业务。我听说你们
之前给中国银行做过类似的项目，我想了解一下那个项目的情况。另外，我们的预算大概
在50万左右，想知道能不能定制开发。对了，我们希望3个月内上线，不知道时间上来不来得及。
还有，我们对数据安全特别重视，想了解你们的数据加密方案..."

期望提取:
- 主意图: 产品咨询
- 关键点: [查询历史案例, 预算50万, 交付周期3个月, 数据安全要求]
- 实体: [金融科技公司, 中国银行, 供应链金融]
- 简化query: "查询中国银行供应链金融项目案例，预算50万，3个月交付，需要数据加密方案"
```

#### 场景2：多问题组合（连续提问）
```
原始输入（180字）:
"我想问几个问题。第一个，你们的产品支持哪些数据库？我们现在用的是Oracle和MySQL。
第二个问题是关于性能的，你们能支持多少并发？我们预计高峰期会有5000个并发用户。
第三个是部署方式，可以私有化部署吗？我们对数据隐私要求比较高。最后一个问题，
你们的技术支持是怎么样的？是7x24小时的吗？哦对了，我还想知道价格是按什么模式收费的。"

期望提取:
- 主意图: 技术咨询（多问题）
- 关键点: [数据库支持, 并发性能, 私有化部署, 技术支持, 收费模式]
- 分解query:
  1. "支持的数据库类型：Oracle, MySQL"
  2. "并发性能：5000用户"
  3. "私有化部署支持"
  4. "技术支持服务等级"
  5. "收费模式"
```

#### 场景3：冗余信息过滤
```
原始输入（150字）:
"嗯...我想问一下，就是那个...怎么说呢...我们公司最近在看你们的产品，
对，就是那个数据分析平台。我之前在朋友那里听说过，他说还不错。
我们现在用的是另一家公司的产品，但是有点贵，而且...怎么说呢，
功能不是特别满意。所以想了解一下你们的产品价格和功能，对比一下。
主要是想做客户画像分析，这个你们能支持吗？"

期望提取:
- 过滤: "嗯", "怎么说呢", "对", "就是那个"等口语化表达
- 简化query: "数据分析平台价格和功能咨询，需要客户画像分析功能"
```

### 3.2 评估指标

| 指标 | 说明 | 计算方法 |
|------|------|---------|
| **信息保留率** | 关键信息是否被提取 | 提取的关键点数 / 人工标注的关键点数 |
| **检索精度** | RAG是否召回正确文档 | Recall@5, Precision@5 |
| **响应准确度** | LLM回复是否准确 | LLM评分（1-10分） |
| **处理延迟** | 总耗时 | 总结时间 + 检索时间 + 生成时间 |
| **Token消耗** | 成本评估 | 总结消耗 + 生成消耗 |

### 3.3 对比实验

| 实验组 | 方法 | Query长度 | 预期效果 |
|-------|------|----------|---------|
| **Baseline** | 直接RAG | 原始长文本（200字） | 检索精度低 |
| **Exp1** | 简单摘要 | LLM总结（50字） | 检索精度提升 |
| **Exp2** | 结构化提取 | 提取关键点+简化query（80字） | 检索精度+信息完整性 ⭐ |
| **Exp3** | 多query检索 | 分解为多个子query | 召回率高但可能有噪音 |

---

## 四、技术实现

### 4.1 分段总结Prompt

```python
SUMMARIZATION_PROMPT = """
你是一个专业的语音助手，用户刚刚进行了一段较长的语音输入。
请分析用户的输入，提取关键信息。

用户输入：
{user_input}

请按照以下格式返回JSON：
{{
  "main_intent": "主要意图（产品咨询/技术咨询/客户查询等）",
  "key_points": ["关键点1", "关键点2", "关键点3"],
  "entities": ["提到的实体1", "实体2"],
  "constraints": ["预算/时间/技术约束"],
  "concise_query": "简化后的查询语句（50-100字）"
}}

注意：
1. 过滤口语化表达（嗯、那个、就是等）
2. 保留核心信息（数字、专有名词、关键需求）
3. 如果有多个问题，需要在key_points中列出
"""
```

### 4.2 核心处理流程

```python
class LongAudioRAGPipeline:
    """长语音输入的RAG处理流程"""

    def __init__(self, llm_client, rag_system):
        self.llm = llm_client
        self.rag = rag_system

    def process(self, long_text: str) -> Dict:
        """处理长文本输入"""
        start_time = time.time()

        # 1. 结构化提取
        structured = self.extract_structure(long_text)
        extract_time = time.time() - start_time

        # 2. RAG检索
        rag_start = time.time()
        if structured["is_multi_question"]:
            # 多问题：分别检索后合并
            results = self.multi_query_retrieval(structured["queries"])
        else:
            # 单问题：直接检索
            results = self.rag.query(structured["concise_query"])
        rag_time = time.time() - rag_start

        # 3. LLM生成回复
        gen_start = time.time()
        response = self.generate_response(
            original_text=long_text,
            structured=structured,
            rag_results=results
        )
        gen_time = time.time() - gen_start

        return {
            "response": response,
            "structured": structured,
            "timing": {
                "extract": extract_time,
                "rag": rag_time,
                "generate": gen_time,
                "total": time.time() - start_time
            }
        }

    def extract_structure(self, text: str) -> Dict:
        """使用LLM提取结构化信息"""
        prompt = SUMMARIZATION_PROMPT.format(user_input=text)
        response = self.llm.chat.completions.create(
            model="Qwen/Qwen3-8B",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)

    def multi_query_retrieval(self, queries: List[str]) -> List[Dict]:
        """多查询检索并去重"""
        all_results = []
        seen_ids = set()

        for query in queries:
            results = self.rag.query(query, top_k=3)
            for doc in results:
                if doc["id"] not in seen_ids:
                    all_results.append(doc)
                    seen_ids.add(doc["id"])

        # 重排序
        if len(all_results) > 5:
            all_results = self.rag.rerank(queries[0], all_results, top_k=5)

        return all_results
```

### 4.3 模拟长语音输入

由于我们没有实际的语音输入系统（VAD + ASR），我们需要模拟这个过程：

```python
class LongAudioSimulator:
    """模拟长语音输入场景"""

    @staticmethod
    def simulate_streaming_input(text: str, chunk_size: int = 50):
        """
        模拟流式输入

        Args:
            text: 完整文本
            chunk_size: 每次返回的字符数
        """
        words = text.split()
        current = ""

        for word in words:
            current += word + " "
            if len(current) >= chunk_size:
                yield current
                current = ""

        if current:
            yield current

    @staticmethod
    def simulate_vad_completion(text: str, pause_threshold: int = 3):
        """
        模拟VAD检测完成

        返回：是否检测到长停顿（意味着用户说完了）
        """
        # 简单模拟：检查是否有句号、问号等终止符
        return text.strip().endswith(('。', '？', '！', '.', '?', '!'))
```

---

## 五、实验步骤

### 5.1 准备工作

1. **准备测试数据**
   - 创建10个长文本测试case
   - 人工标注关键信息和期望结果

2. **搭建基础设施**
   - 确保vLLM服务运行
   - 准备RAG知识库（复用实验2的数据）

### 5.2 实验流程

```bash
# 1. 启动vLLM服务（如未启动）
cd ~/tts
bash scripts/start_local_services.sh

# 2. 运行实验3
python experiments/test_03_long_audio_rag.py

# 3. 查看结果
cat outputs/experiment3_long_audio_results_*.json
cat outputs/experiment3_long_audio_report_*.md
```

### 5.3 输出内容

**JSON结果文件**:
```json
{
  "test_case_id": "complex_product_inquiry",
  "original_text": "...",
  "original_length": 218,
  "structured_query": {
    "main_intent": "产品咨询",
    "key_points": [...],
    "concise_query": "..."
  },
  "rag_results": [...],
  "final_response": "...",
  "metrics": {
    "info_retention_rate": 0.95,
    "rag_recall_at_5": 0.8,
    "response_score": 8.5,
    "total_latency": 2.34,
    "token_usage": {
      "summarization": 456,
      "generation": 821
    }
  }
}
```

---

## 六、预期结果

### 6.1 性能预期

| 指标 | Baseline（直接RAG） | 分段总结方案 | 改进 |
|------|-------------------|-------------|------|
| 信息保留率 | 60% | **90%** | +30% |
| RAG召回率 | 40% | **75%** | +35% |
| 响应准确度 | 6.5/10 | **8.5/10** | +2分 |
| 处理延迟 | 1.5s | **2.5s** | +1s |
| Token消耗 | 500 | **1200** | +140% |

### 6.2 关键发现（预测）

1. ✅ **分段总结显著提升检索精度**
   - 长query包含过多噪音
   - 简化后的query更聚焦

2. ✅ **结构化提取保留关键信息**
   - 避免信息丢失
   - 便于后续处理

3. ⚠️ **延迟增加可接受**
   - 总结增加1s延迟
   - 但换来更高准确性

4. ⚠️ **成本增加需要权衡**
   - 额外的LLM调用
   - 可以通过缓存优化

---

## 七、优化方向

### 7.1 短期优化

1. **Prompt优化**
   - 更精确的提取指令
   - Few-shot示例

2. **缓存机制**
   - 相似query缓存
   - 避免重复总结

3. **并行处理**
   - 总结和初步检索并行
   - 减少延迟

### 7.2 长期优化

1. **小模型提取**
   - 使用专门的小模型做提取
   - 降低成本

2. **渐进式总结**
   - 实现方案A（实时总结）
   - 提升用户体验

3. **多模态融合**
   - 结合语音韵律信息
   - 识别重点和情感

---

## 八、文件结构

```
experiments/
├── EXPERIMENT3_LONG_AUDIO_DESIGN.md       # 本设计文档
├── test_03_long_audio_rag.py              # 测试脚本
├── long_audio_test_cases.json             # 测试用例
└── long_audio_simulator.py                # 长语音模拟器

outputs/
├── experiment3_long_audio_results_*.json  # 测试结果
└── experiment3_long_audio_report_*.md     # 分析报告
```

---

## 九、参考资料

1. **Incremental Summarization**: [MapReduce for Long Documents](https://arxiv.org/abs/2401.xxxxx)
2. **Query Decomposition**: Multi-hop Question Answering
3. **RAG with Long Context**: LongRAG Paper

---

**文档版本**: v1.0
**最后更新**: 2025-12-20
**作者**: Haoquan + Claude Sonnet 4.5
