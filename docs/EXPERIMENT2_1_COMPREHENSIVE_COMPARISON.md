# 实验1完整对比：Qwen3-8B / 14B / 32B 性能对比

**实验日期**: 2025-12-20
**服务器**: Azure A100 (4 × 80GB)
**vLLM 版本**: 0.12.0

---

## 一、实验配置

| 模型 | 参数量 | 部署方式 | GPU 使用 | 显存占用 |
|------|--------|----------|----------|---------|
| **Qwen3-8B** | 8B | 单卡 | 1 × A100 | ~20 GB |
| **Qwen3-14B** | 14B | 单卡 | 1 × A100 | ~35 GB |
| **Qwen3-32B** | 32B | 双卡 TP | 2 × A100 | ~72 GB (分布式) |

**测试场景**: 6 个（复杂多需求、技术对比、ROI计算、陷阱问题、近似匹配、近义词区分）
**测试模式**: 2 种（无 RAG / 有 RAG）
**总测试数**: 每个模型 12 次测试

---

## 二、性能对比（实测数据）

### 2.1 生成速度对比

| 模型 | 生成速度 | vs 8B | 部署方式 |
|------|---------|-------|---------|
| **Qwen3-8B** | **72.64 tokens/s** | 基准 | 单卡 ✅ |
| **Qwen3-14B** | **44.33 tokens/s** | -39% | 单卡 ✅ |
| **Qwen3-32B** | **36.20 tokens/s** | -50% | 双卡 TP ⚠️ |

**关键发现**:
- ✅ **8B 速度最快**，适合高频实时场景
- ⚠️ **32B 反而比 14B 慢**，原因是双卡通信开销（Tensor Parallelism）
- 📊 模型越大，速度越慢（符合预期）

### 2.2 延迟对比

| 模型 | 首 Token 延迟 | vs 8B | 部署方式 |
|------|--------------|-------|---------|
| **Qwen3-8B** | **0.754s** | 基准 | 单卡 ✅ |
| **Qwen3-14B** | **0.902s** | +20% | 单卡 ✅ |
| **Qwen3-32B** | **0.938s** | +24% | 双卡 TP ⚠️ |

**说明**: 首 Token 延迟主要受模型大小和跨卡通信影响。

### 2.3 准确性对比

#### 无 RAG 场景

| 模型 | 平均得分 | vs 8B | 相对提升 |
|------|---------|-------|---------|
| **Qwen3-8B** | **63.81** | 基准 | - |
| **Qwen3-14B** | **62.66** | -1.8% | 几乎相同 ⚠️ |
| **Qwen3-32B** | **60.54** | -5.1% | 反而下降 ⚠️ |

#### 有 RAG 场景

| 模型 | 平均得分 | vs 8B | 相对提升 |
|------|---------|-------|---------|
| **Qwen3-8B** | **69.67** | 基准 | - |
| **Qwen3-14B** | **71.82** | +3.1% | 略有提升 ✅ |
| **Qwen3-32B** | **68.56** | -1.6% | 几乎相同 ⚠️ |

#### RAG 提升效果

| 模型 | 无 RAG | 有 RAG | RAG 提升 |
|------|--------|--------|---------|
| **Qwen3-8B** | 63.81 | 69.67 | **+5.86** |
| **Qwen3-14B** | 62.66 | 71.82 | **+9.15** |
| **Qwen3-32B** | 60.54 | 68.56 | **+8.02** |

**关键发现**:
- 🤔 **32B 准确性未明显高于 8B/14B**（可能是测试场景偏简单）
- ✅ **RAG 对所有模型都有显著提升**（+5.86 ~ +9.15）
- ✅ **14B + RAG 得分最高** (71.82)

---

## 三、性价比分析

### 3.1 速度 vs 准确性权衡

```
              速度         准确性(RAG)    性价比
8B:    ████████████████  (72.6 t/s)  69.67 ✅✅✅✅✅
14B:   ███████████       (44.3 t/s)  71.82 ✅✅✅✅
32B:   █████████         (36.2 t/s)  68.56 ✅✅
```

### 3.2 成本效益比较

假设场景：日调用 10 万次，每次生成 200 tokens

| 模型 | GPU 数 | 吞吐量 | 并发能力 | 硬件成本 | 推荐场景 |
|------|--------|--------|---------|---------|---------|
| **8B** | 1 | 高 | 5-10路 | 1× | ⭐⭐⭐⭐⭐ 高频场景 |
| **14B** | 1 | 中 | 2-4路 | 1× | ⭐⭐⭐⭐ 平衡场景 |
| **32B** | 2 | 低 | 1-2路 | 2× | ⭐⭐ 仅高准确性场景 |

**结论**:
- 💰 **8B 性价比最高** - 速度快，成本低，准确性可接受
- ⚖️ **14B 平衡点** - 准确性最高，速度尚可
- ⚠️ **32B 不推荐** - 在当前测试场景下，准确性提升有限，但成本和延迟显著增加

---

## 四、关键技术发现

### 4.1 Tensor Parallelism 性能损失

**观察**: 32B 双卡 (TP=2) 速度 (36.2 tokens/s) 比 14B 单卡 (44.3 tokens/s) 还慢

**原因**:
1. **跨卡通信开销**: GPU 间需要同步中间结果（All-Reduce）
2. **带宽限制**: PCIe 通信带宽远低于 GPU 内存带宽
3. **同步等待**: 两张卡需要等待最慢的那张完成

**计算**:
- 理论上 32B 参数量是 14B 的 2.3×，单卡速度应该约为 14B 的 1/2.3 = 43%
- 实际测得 32B 速度是 14B 的 82% (36.2 / 44.3)
- **TP 通信损失约 18%**

### 4.2 单卡 vs 双卡对比

| 指标 | 14B 单卡 | 32B 双卡 | 结论 |
|------|---------|---------|------|
| 生成速度 | 44.3 tokens/s | 36.2 tokens/s | 单卡更快 ✅ |
| 首 token 延迟 | 0.902s | 0.938s | 单卡更低 ✅ |
| 准确性 (RAG) | 71.82 | 68.56 | 单卡更高 ✅ |
| GPU 数量 | 1 | 2 | 单卡成本低 ✅ |

**结论**: **在 A100 80GB 上，14B 单卡优于 32B 双卡**

### 4.3 为什么 32B 准确性未显著提升？

可能原因：
1. **测试场景相对简单** - 当前 6 个场景对 8B/14B 已足够
2. **RAG 效果**  - RAG 提供了外部知识，降低了对模型本身能力的依赖
3. **评分标准** - 关键词匹配评分可能无法充分体现复杂推理能力
4. **样本数量少** - 每个模型只测试了 12 次（6 场景 × 2 模式）

**建议**: 需要更复杂的测试场景（如多步推理、复杂逻辑）才能体现大模型优势

---

## 五、实际应用推荐

### 场景 1: 客服机器人（高频 + 实时）
```
推荐: Qwen3-8B (单卡)
理由:
- 72.6 tokens/s 速度，满足实时响应
- 准确性 69.67 足够应对常见问题
- 成本最低，支持高并发
硬件: 1 × A100 80GB
```

### 场景 2: 技术文档问答（中频 + 复杂）
```
推荐: Qwen3-14B (单卡)
理由:
- 准确性最高 (71.82)
- 速度 44.3 tokens/s 可接受
- 单卡部署，简单高效
硬件: 1 × A100 80GB
```

### 场景 3: 代码生成/复杂推理（低频 + 高质量）
```
推荐: Qwen3-32B (双卡) 或 云端 72B
理由:
- 当前测试场景下 32B 优势不明显
- 建议使用云端 API (72B) 或本地微调 14B
- 如需隐私，考虑 32B 双卡
硬件: 2 × A100 80GB (如必须本地)
```

### 场景 4: 混合部署（推荐 ⭐⭐⭐）
```
方案: 8B (80%) + 云端 72B (20%)
理由:
- 简单问题用 8B（快速、便宜）
- 复杂问题动态路由到云端 72B
- 成本最优，体验最佳

实现:
1. 实时评估问题复杂度
2. 简单问题 → 本地 8B
3. 复杂问题 → 云端 72B
```

---

## 六、最终结论

### 6.1 性能排名

**生成速度**:
1. 🥇 **Qwen3-8B**: 72.6 tokens/s (单卡)
2. 🥈 **Qwen3-14B**: 44.3 tokens/s (单卡)
3. 🥉 **Qwen3-32B**: 36.2 tokens/s (双卡 TP)

**准确性 (有 RAG)**:
1. 🥇 **Qwen3-14B**: 71.82 (单卡)
2. 🥈 **Qwen3-8B**: 69.67 (单卡)
3. 🥉 **Qwen3-32B**: 68.56 (双卡 TP)

**性价比**:
1. 🥇 **Qwen3-8B**: ⭐⭐⭐⭐⭐
2. 🥈 **Qwen3-14B**: ⭐⭐⭐⭐
3. 🥉 **Qwen3-32B**: ⭐⭐

### 6.2 关键建议

1. **✅ 优先推荐 Qwen3-8B (单卡)**
   - 速度最快，成本最低
   - 配合 RAG 后准确性可接受
   - 适合 80% 的应用场景

2. **✅ 考虑 Qwen3-14B (单卡)**
   - 准确性最高
   - 速度可接受
   - 适合对质量要求高的场景

3. **⚠️ 慎用 Qwen3-32B (双卡)**
   - 当前测试场景下优势不明显
   - TP 通信开销显著（-18% 速度）
   - 仅在极高准确性要求时考虑

4. **🚫 不推荐单卡部署 32B**
   - 会 OOM（显存不足）
   - 必须使用双卡 TP

### 6.3 技术洞察

**Tensor Parallelism 的代价**:
- 速度损失：~18%（通信开销）
- 延迟增加：~4%
- 部署复杂度：显著增加

**RAG 的价值**:
- 对所有模型都有显著提升（+5.86 ~ +9.15）
- 能让小模型逼近大模型能力
- 性价比远高于单纯升级模型

**单卡优势**:
- 无跨卡通信，速度更快
- 部署简单，维护成本低
- A100 80GB 足以支持 14B 模型

---

## 七、附录：详细测试数据

### 测试文件
- 8B 结果: `experiment1_results_20251220_125747.json`
- 14B 结果: `experiment1_results_20251220_130129.json`
- 32B 结果: `experiment1_results_20251220_130810.json`

### 测试环境
- GPU: NVIDIA A100 80GB PCIe × 4
- CPU: 96 核
- vLLM: 0.12.0
- Python: 3.13.11
- CUDA: 13.1

### 配置参数
```bash
# 8B / 14B (单卡)
--model Qwen/Qwen3-{8B,14B}
--gpu-memory-utilization 0.90
--max-model-len 32768
--reasoning-parser qwen3

# 32B (双卡)
--model Qwen/Qwen3-32B
--tensor-parallel-size 2
--gpu-memory-utilization 0.90
--max-model-len 32768
--reasoning-parser qwen3
```

---

**报告生成时间**: 2025-12-20
**作者**: Haoquan + Claude Sonnet 4.5
